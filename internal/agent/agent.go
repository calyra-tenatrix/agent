package agent

import (
	"context"
	"fmt"
	"log"
	"sync"
	"time"

	"github.com/calyra-tenatrix/agent/internal/cloud"
	"github.com/calyra-tenatrix/agent/internal/config"
	agentv1 "github.com/calyra-tenatrix/agent/internal/proto/agent/v1"
	"github.com/calyra-tenatrix/agent/internal/reconciler"
	"github.com/calyra-tenatrix/agent/internal/system"
)

// Agent represents the main agent structure
type Agent struct {
	config      *config.Config
	version     string
	client      *GRPCClient
	sysInfo     *SystemInfo
	provisioner *cloud.ClusterProvisioner // Manages K8s cluster lifecycle
	reconciler  *reconciler.Reconciler    // Set by provisioner when cluster is ready
	ctx         context.Context           // Store context for async operations

	// Provisioning state
	provisionMu    sync.Mutex
	isProvisioning bool
}

// SystemInfo is an alias for system.SystemInfo
type SystemInfo = system.SystemInfo

// Run starts the agent
func Run(ctx context.Context, cfg *config.Config, version string) error {
	agent := &Agent{
		config:  cfg,
		version: version,
	}

	return agent.run(ctx)
}

func (a *Agent) run(ctx context.Context) error {
	log.Printf("üöÄ Tenatrix Agent v%s starting...", a.version)
	a.ctx = ctx

	// Create gRPC client
	client, err := NewGRPCClient(a.config.BackendURL, a.config.TargetToken, a.version)
	if err != nil {
		return fmt.Errorf("failed to create gRPC client: %w", err)
	}
	a.client = client
	defer a.client.Close()

	// Connect to backend
	if err := a.client.Connect(ctx); err != nil {
		return fmt.Errorf("failed to connect to backend: %w", err)
	}

	// Initialize cluster provisioner (lazy - cluster created on first deployment)
	if err := a.initProvisioner(); err != nil {
		log.Printf("‚ö†Ô∏è  Cluster provisioner not available: %v", err)
		// Continue without provisioner - will fail on reconcile tasks
	} else {
		log.Printf("‚úÖ Cluster provisioner initialized (cluster will be created on first deployment)")
	}

	// Collect initial system info
	sysInfo, err := system.Collect()
	if err != nil {
		log.Printf("‚ö†Ô∏è  Failed to collect system info: %v", err)
		sysInfo = &SystemInfo{} // Use empty info
	}
	a.sysInfo = sysInfo

	log.Printf("üìä System Info: %s | %s | IP: %s | Region: %s",
		sysInfo.Hostname, sysInfo.OSVersion, sysInfo.IPAddress, sysInfo.CloudRegion)

	// Open bidirectional streaming channel
	_, err = a.client.OpenChannel(ctx)
	if err != nil {
		return fmt.Errorf("failed to open channel: %w", err)
	}

	// Send initial status
	if err := a.client.SendInitialStatus(ctx, sysInfo); err != nil {
		return fmt.Errorf("failed to send initial status: %w", err)
	}

	// Start heartbeat goroutine
	go a.heartbeatLoop(ctx)

	// Start message receiver
	log.Printf("‚úÖ Agent is ready and listening for messages")
	if err := a.client.ReceiveMessages(ctx, a); err != nil {
		return fmt.Errorf("message receiver failed: %w", err)
	}

	return nil
}

// initProvisioner initializes the cluster provisioner
// The actual K8s cluster is created lazily on first reconcile task
func (a *Agent) initProvisioner() error {
	// Extract target ID from token (format: uuid:signature)
	targetID := a.config.TargetToken
	if idx := len(targetID); idx > 36 {
		targetID = targetID[:36] // Get just the UUID part
	}

	provisioner, err := cloud.NewClusterProvisioner(a.ctx, cloud.ProvisionerConfig{
		DOToken:   a.config.DOToken,
		TargetID:  targetID,
		Namespace: a.config.KubernetesNamespace,
		StatusCallback: func(status cloud.ProvisioningStatus) {
			// Log provisioning status
			log.Printf("[Provisioner] %s: %s (progress: %d%%)",
				status.State, status.Message, status.Progress)
			// TODO: Send status to backend via gRPC
		},
	})
	if err != nil {
		return fmt.Errorf("failed to create provisioner: %w", err)
	}

	a.provisioner = provisioner

	// Check if cluster already exists and is ready
	if provisioner.IsClusterReady() {
		a.reconciler = provisioner.GetReconciler()
		log.Printf("‚úÖ Existing Kubernetes cluster connected")
	}

	return nil
}

// ensureCluster ensures the K8s cluster is ready before reconciliation
// This is called on-demand when a reconcile task is received
func (a *Agent) ensureCluster() (*reconciler.Reconciler, error) {
	a.provisionMu.Lock()
	defer a.provisionMu.Unlock()

	// If already provisioning, wait
	if a.isProvisioning {
		return nil, fmt.Errorf("cluster provisioning already in progress")
	}

	// If reconciler is already available, return it
	if a.reconciler != nil {
		return a.reconciler, nil
	}

	// Check provisioner
	if a.provisioner == nil {
		return nil, fmt.Errorf("cluster provisioner not initialized")
	}

	a.isProvisioning = true
	defer func() { a.isProvisioning = false }()

	log.Printf("üöÄ First deployment - provisioning Kubernetes cluster...")

	// This may take 5-10 minutes for new cluster
	rec, err := a.provisioner.EnsureCluster(a.ctx)
	if err != nil {
		return nil, fmt.Errorf("failed to provision cluster: %w", err)
	}

	a.reconciler = rec
	return rec, nil
}

// heartbeatLoop sends periodic heartbeats
func (a *Agent) heartbeatLoop(ctx context.Context) {
	ticker := time.NewTicker(a.config.HeartbeatInterval)
	defer ticker.Stop()

	log.Printf("üíì Heartbeat started (interval: %s)", a.config.HeartbeatInterval)

	for {
		select {
		case <-ctx.Done():
			log.Printf("üíì Heartbeat stopped")
			return
		case <-ticker.C:
			// Collect fresh system info
			sysInfo, err := system.Collect()
			if err != nil {
				log.Printf("‚ö†Ô∏è  Failed to collect system info for heartbeat: %v", err)
				continue
			}
			a.sysInfo = sysInfo

			// Send heartbeat
			if err := a.client.SendHeartbeat(ctx, sysInfo); err != nil {
				log.Printf("‚ö†Ô∏è  Heartbeat failed: %v", err)
				continue
			}

			log.Printf("üíì Heartbeat sent (CPU: %.1f%%, Mem: %.1f%%, Disk: %.1f%%)",
				sysInfo.CPUPercent, sysInfo.MemoryPercent, sysInfo.DiskPercent)
		}
	}
}

// MessageHandler implementation

func (a *Agent) HandleWelcome(msg *agentv1.WelcomeMessage) error {
	log.Printf("üëã Welcome message from backend: %s (server version: %s)",
		msg.Message, msg.ServerVersion)
	return nil
}

func (a *Agent) HandleTask(task *agentv1.Task) error {
	log.Printf("üìã Received task: %s (type: %s)", task.Id, task.Type)
	// TODO: Implement task execution in future versions
	log.Printf("‚ö†Ô∏è  Task execution not implemented yet (V1 only has heartbeat)")
	return nil
}

func (a *Agent) HandleConfigUpdate(update *agentv1.ConfigUpdate) error {
	log.Printf("‚öôÔ∏è  Received config update: version %s", update.Version)
	// TODO: Implement config update in future versions
	log.Printf("‚ö†Ô∏è  Config update not implemented yet")
	return nil
}

func (a *Agent) HandleTaskCancellation(cancellation *agentv1.TaskCancellation) error {
	log.Printf("‚ùå Task cancellation: %s (reason: %s)", cancellation.TaskId, cancellation.Reason)
	// TODO: Implement task cancellation in future versions
	return nil
}

func (a *Agent) HandleHealthCheck(req *agentv1.HealthCheckRequest) error {
	log.Printf("üè• Received health check request: %s", req.RequestId)
	// TODO: Respond with health status via stream
	return nil
}

func (a *Agent) HandleUpdateNotification(notification *agentv1.UpdateNotification) error {
	log.Printf("üîÑ Update available: %s -> %s", notification.CurrentVersion, notification.NewVersion)
	log.Printf("   Download URL: %s", notification.DownloadUrl)
	log.Printf("   Checksum: %s", notification.Checksum)

	if notification.ForceUpdate {
		log.Printf("‚ö†Ô∏è  FORCED UPDATE - Agent will update automatically")
		// TODO: Trigger auto-update mechanism
	} else {
		log.Printf("‚ÑπÔ∏è  Optional update available (will check during next update cycle)")
	}

	return nil
}

func (a *Agent) HandleReconcileTask(task *agentv1.ReconcileTask) error {
	log.Printf("üîÑ Received reconcile task: %s (app: %s, env: %s, dry_run: %v)",
		task.TaskId,
		task.Desired.ApplicationId,
		task.Desired.Environment,
		task.DryRun)

	// Execute reconciliation asynchronously to not block message processing
	go func() {
		// Ensure cluster is ready (creates on first deployment)
		rec, err := a.ensureCluster()
		if err != nil {
			log.Printf("‚ùå Failed to ensure cluster: %v", err)
			result := &agentv1.ReconcileResult{
				TaskId:  task.TaskId,
				Success: false,
				Error: &agentv1.ErrorInfo{
					Code:    "CLUSTER_PROVISIONING_FAILED",
					Message: fmt.Sprintf("Failed to provision Kubernetes cluster: %v", err),
				},
			}
			if sendErr := a.client.SendReconcileResult(result); sendErr != nil {
				log.Printf("‚ùå Failed to send reconcile result: %v", sendErr)
			}
			return
		}

		log.Printf("‚öôÔ∏è  Starting reconciliation for task %s...", task.TaskId)

		// Execute reconciliation
		result := rec.ReconcileTask(a.ctx, task)

		// Log result
		if result.Success {
			log.Printf("‚úÖ Reconciliation task %s completed successfully", task.TaskId)
			if result.DryRun {
				log.Printf("   üìã Planned changes: %d", len(result.PlannedChanges))
			} else {
				log.Printf("   üìã Applied changes: %d", len(result.AppliedChanges))
			}
		} else {
			log.Printf("‚ùå Reconciliation task %s failed: %s", task.TaskId, result.Error.Message)
		}

		// Send result back to backend
		if err := a.client.SendReconcileResult(result); err != nil {
			log.Printf("‚ùå Failed to send reconcile result: %v", err)
		}
	}()

	return nil
}
